{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import DatasetDict, Dataset\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'source', 'language', 'label'],\n",
      "    num_rows: 270399\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'label', 'emotion'],\n",
      "    num_rows: 16000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load your sentiment dataset again (assuming you saved it)\n",
    "sentiment_df = pd.read_csv(\"../data/sentiment_dataset.csv\")\n",
    "emotion_df = pd.read_csv(\"../data/emotion_dataset.csv\")\n",
    "\n",
    "# Convert sentiment DataFrame to Hugging Face Dataset\n",
    "sentiment_dataset_hf = Dataset.from_pandas(sentiment_df)\n",
    "\n",
    "# Convert emotion DataFrame to Hugging Face Dataset\n",
    "emotion_dataset_hf = Dataset.from_pandas(emotion_df)\n",
    "\n",
    "print(sentiment_dataset_hf)\n",
    "print(emotion_dataset_hf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'source', 'language', 'label'],\n",
      "        num_rows: 216319\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'source', 'language', 'label'],\n",
      "        num_rows: 27040\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'source', 'language', 'label'],\n",
      "        num_rows: 27040\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'emotion'],\n",
      "        num_rows: 12800\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label', 'emotion'],\n",
      "        num_rows: 1600\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'emotion'],\n",
      "        num_rows: 1600\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Sentiment dataset split\n",
    "sentiment_dataset_hf = sentiment_dataset_hf.train_test_split(test_size=0.2, seed=42)\n",
    "sentiment_valid_test = sentiment_dataset_hf['test'].train_test_split(test_size=0.5, seed=42)\n",
    "sentiment_datasets = DatasetDict({\n",
    "    'train': sentiment_dataset_hf['train'],\n",
    "    'validation': sentiment_valid_test['train'],\n",
    "    'test': sentiment_valid_test['test']\n",
    "})\n",
    "\n",
    "# Emotion dataset split\n",
    "emotion_dataset_hf = emotion_dataset_hf.train_test_split(test_size=0.2, seed=42)\n",
    "emotion_valid_test = emotion_dataset_hf['test'].train_test_split(test_size=0.5, seed=42)\n",
    "emotion_datasets = DatasetDict({\n",
    "    'train': emotion_dataset_hf['train'],\n",
    "    'validation': emotion_valid_test['train'],\n",
    "    'test': emotion_valid_test['test']\n",
    "})\n",
    "\n",
    "# Verify the splits\n",
    "print(sentiment_datasets)\n",
    "print(emotion_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce2dd17d01e4725a39a50b45d2b3ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\multilingual-transformer-sentiment-emotion-5vIiU82Q-py3.12\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Lenovo\\.cache\\huggingface\\hub\\models--xlm-roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "429cbef03a8b439a9e673f63da489dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0197e8f666f74ad590bf73056329de8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9918f24e1f9b40ef92259a6d3e241c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36b1c09d2c34c1c903483a9082ec27c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/216319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df820340660400fbb93ddb47ad7f690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9b3f9e9c6e4ca4b38dbb6559da3e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e55bf6a16754145af508050fcc21746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2150449785f54a3680c695e9256175e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11feabb33dcd4ee99eeae74cae3a9cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'ハンズフリーイヤフォン・kindle・スマホ・Bluetoothスピーカーなどを充電するとき、複数のアダプタを使用するのは面倒なので購入しました。同時使用機器数を考慮し３ポートの本機を購入しました。他の同種の商品で折り畳みプラグ方式の耐久性に難ありとのamazonレビューを参考に固定式プラグをチョイスしました。 スマホが急速充電中にはそれなりの電流値が表示されているようです。今のところ快適に使用しています。 プラグの耐久性は心配ないので、これから先、回路が長期間の仕様に耐えてくれることを期待しています。', 'source': 'amazon_reviews_multi', 'language': 'japanese', 'label': 0, 'input_ids': [0, 6, 158676, 13591, 119415, 161996, 100537, 7740, 1925, 68347, 133, 1925, 72040, 1925, 114567, 188, 29827, 3385, 78694, 48275, 45485, 97047, 1786, 32658, 37, 130159, 6242, 27857, 16985, 10793, 187014, 6814, 145192, 15871, 32299, 9575, 30, 16324, 2229, 74086, 4723, 251, 49162, 1373, 363, 177799, 154, 1516, 3752, 156530, 9575, 30, 27329, 3169, 10694, 154, 6689, 507, 16059, 2694, 244417, 6095, 133628, 21300, 6518, 154, 26361, 15954, 1278, 327, 10648, 41792, 18513, 54149, 191, 168788, 215103, 41284, 5392, 133628, 21300, 251, 178334, 94586, 9575, 30, 6, 72040, 281, 222163, 97047, 514, 2880, 24054, 57233, 154, 6312, 4695, 45975, 151271, 7826, 25698, 30, 58643, 17036, 4771, 19807, 327, 2229, 8209, 30, 6, 133628, 21300, 154, 26361, 15954, 1278, 342, 61357, 63644, 37, 56741, 2996, 37, 2490, 3136, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'text': 'i feel like god has been gracious in answering prayers', 'label': 1, 'emotion': 'joy', 'input_ids': [0, 17, 12319, 1884, 2355, 1556, 2809, 4224, 60744, 23, 35166, 214, 177966, 7, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(batch['text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Tokenize sentiment dataset\n",
    "tokenized_sentiment = sentiment_datasets.map(tokenize_batch, batched=True)\n",
    "\n",
    "# Tokenize emotion dataset\n",
    "tokenized_emotion = emotion_datasets.map(tokenize_batch, batched=True)\n",
    "\n",
    "# Check a sample tokenized entry\n",
    "print(tokenized_sentiment['train'][0])\n",
    "print(tokenized_emotion['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentiment.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "tokenized_emotion.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e9da383c874debbe1f713e16758a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/216319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b68585e481e45adac932109adadaa04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/27040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690bfb61333a44d5be0f8dfc82029eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/27040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e48923d17b0491ba524bc455808534b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/12800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d672416f7e14509ba90ea927bdfda62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647fc8b8a8d74273b037d6200b8eccab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_sentiment.save_to_disk(\"../data/tokenized_sentiment\")\n",
    "tokenized_emotion.save_to_disk(\"../data/tokenized_emotion\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'source', 'language', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 216319\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'source', 'language', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 27040\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'source', 'language', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 27040\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'emotion', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 12800\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label', 'emotion', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1600\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'emotion', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1600\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_sentiment)\n",
    "print(tokenized_emotion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multilingual-transformer-sentiment-emotion-5vIiU82Q-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
